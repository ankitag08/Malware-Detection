import pandas as pd
import numpy as np
import ipaddress
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
import sklearn.metrics as sm
import sys
import matplotlib as mp
import seaborn as sb
from sklearn.preprocessing import Normalizer
from sklearn.naive_bayes import GaussianNB
from sklearn import linear_model
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import sklearn.ensemble as ske
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import scikitplot as skplt
import matplotlib.pyplot as plt
from sklearn.metrics import plot_confusion_matrix
import warnings
warnings.filterwarnings('ignore')

np.random.seed(1337)
data=pd.read_csv(r"C:\\Users\Ankita Gupta\Desktop\Major_Project\UNSW_NB15.csv",low_memory=False)
data.info()
print(data.head())
#print(data.isnull().sum())
print(data.shape)
print(data.describe())
print("Variety in Label:")
print(data["label"].value_counts())

for columns in data.columns:
    if(data[columns].dtypes=='object'):
        count=data[columns].nunique()
        print("column Name:",columns)
        print(count)

data_column = ['proto']
data_values = data[data_column]
data_enc=data_values.apply(LabelEncoder().fit_transform)
data['proto'] = np.array(data_enc)

data_column = ['service']
data_values = data[data_column]
data_enc=data_values.apply(LabelEncoder().fit_transform)
data['service'] = np.array(data_enc)

data_column = ['state']
data_values = data[data_column]
data_enc=data_values.apply(LabelEncoder().fit_transform)
data['state'] = np.array(data_enc)

data_column = ['attack_cat']
data_values = data[data_column]
data_enc=data_values.apply(LabelEncoder().fit_transform)
data['attack_cat'] = np.array(data_enc)

data.info()
x = data.drop('label',1)
y = data.label

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.3,random_state=105)

scaler = Normalizer().fit(X_train)
x_train = scaler.transform(X_train)

scaler = Normalizer().fit(X_test)
x_test = scaler.transform(X_test)

#GAUSSIAN_NB
print("GAUSSIAN_NB:")
from sklearn.naive_bayes import GaussianNB
Gb=GaussianNB()
Gb.fit(x_train,y_train)
y_predict = Gb.predict(x_test)
acc = r2_score(y_predict, y_test)
accuracy = accuracy_score(y_test, y_predict)
recall = recall_score(y_test, y_predict , average="macro")
precision = precision_score(y_test,y_predict , pos_label=1, average='macro', sample_weight=None, zero_division=0 )
f1 = f1_score(y_test,y_predict,average="macro")
#print("NaiveBayes (r2 score):-")
#print(acc)
print("Accuracy")
print(accuracy)
print("Precision")
print(precision)
print("Recall")
print(recall)
print("F1score")
print(f1)
#print("Confusion Matrix:")
#print(confusion_matrix(y_test, y_predict))
print("Classification Report:")
print(classification_report(y_test, y_predict))
print("Confusion Matrix(Multilabel):")
print(sm.multilabel_confusion_matrix(y_test, y_predict))
titles_options = [("Confusion matrix, without normalization", None),
                  ("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(Gb, x_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize=normalize)
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)
plt.show()
TN, FP, FN, TP = confusion_matrix(y_test,y_predict).ravel()
TPR = TP/(TP+FN)
TNR = TN/(TN+FP)
FPR = FP/(FP+TN)
FNR = FN/(TP+FN)
print("True Positive Rate:")
print(TPR)
print("True Negative Rate:")
print(TNR)
print("False Positive Rate:")
print(FPR)
print("False Negative Rate:")
print(FNR)



#LINEARMODEL
print("LINEAR_MODEL:")
lm = linear_model.BayesianRidge()
lm.fit(x_train,y_train)
lm_predict=lm.predict(x_test)
lm_predict=lm_predict.round()
acc=r2_score(y_test,lm_predict)
lmaccuracy = accuracy_score(y_test, lm_predict)
lmrecall = recall_score(y_test, lm_predict , average="macro")
lmprecision = precision_score(y_test,lm_predict, pos_label=1, average='macro', sample_weight=None, zero_division=0 )
lmf1 = f1_score(y_test,lm_predict,average="macro")
print("Linear Model (r2 score):-")
print(acc)
print("Accuracy")
print(lmaccuracy)
print("Precision")
print(lmprecision)
print("Recall")
print(lmrecall)
print("F1score")
print(lmf1)
#print("Confusion Matrix:")
#print(confusion_matrix(y_test, y_predict))
print("Classification Report:")
print(classification_report(y_test, lm_predict))
print("Confusion Matrix(Multilabel):")
print(sm.multilabel_confusion_matrix(y_test, lm_predict))
""""
TN, FP, FN, TP = confusion_matrix(y_test,lm_predict).ravel()
TPR = TP/(TP+FN)
TNR = TN/(TN+FP)
FPR = FP/(FP+TN)
FNR = FN/(TP+FN)
print("True Positive Rate:")
print(TPR)
print("True Negative Rate:")
print(TNR)
print("False Positive Rate:")
print(FPR)
print("False Negative Rate:")
print(FNR)
"""

#LOGISTICREGRESSION
print("LOGISTIC_REGRESSION:")
lg = LogisticRegression()
lg.fit(x_train,y_train)
lg_predict=lg.predict(x_test)
lg_predict=lg_predict.round()
acc=r2_score(y_test,lg_predict)
accuracy = accuracy_score(y_test, lg_predict)
recall = recall_score(y_test, lg_predict , average="macro")
precision = precision_score(y_test,lg_predict , pos_label=1, average='macro', sample_weight=None, zero_division=0)
f1 = f1_score(y_test,lg_predict,average="macro")
print("Logistic Regression (r2 score):-")
print(acc)
print("Accuracy")
print(accuracy)
print("Precision")
print(precision)
print("Recall")
print(recall)
print("F1score")
print(f1)
#print("Confusion Matrix:")
#print(confusion_matrix(y_test, lg_predict))
print("Classification Report:")
print(classification_report(y_test, lg_predict))
print("Confusion Matrix(Multilabel):")
print(sm.multilabel_confusion_matrix(y_test, lg_predict))

titles_options = [("Confusion matrix, without normalization", None),
                  ("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(lg, x_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize=normalize)
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)
plt.show()
TN, FP, FN, TP = confusion_matrix(y_test,lg_predict).ravel()
TPR = TP/(TP+FN)
TNR = TN/(TN+FP)
FPR = FP/(FP+TN)
FNR = FN/(TP+FN)
print("True Positive Rate:")
print(TPR)
print("True Negative Rate:")
print(TNR)
print("False Positive Rate:")
print(FPR)
print("False Negative Rate:")
print(FNR)




#KNEIGHBOURCLASSIFIER
print("KNEIGHBOUR_CLASSIFIER:")
knc = KNeighborsClassifier()
knc.fit(x_train,y_train)
knc_predict=knc.predict(x_test)
knc_predict=knc_predict.round()
acc=r2_score(y_test,knc_predict)
accuracy = accuracy_score(y_test, knc_predict)
recall = recall_score(y_test, knc_predict , average="macro")
precision = precision_score(y_test,knc_predict, pos_label=1, average='macro', sample_weight=None, zero_division=0)
f1 = f1_score(y_test,knc_predict,average="macro")
print("KNeighbour Classifier (r2 score):-")
print(acc)
print("Accuracy:")
print(accuracy)
print("Precision:")
print(precision)
print("Recall:")
print(recall)
print("F1score:")
print(f1)
#print("Confusion Matrix:")
#print(confusion_matrix(y_test, knc_predict))
print("Classification Report:")
print(classification_report(y_test, knc_predict))
print("Confusion Matrix(Multilabel):")
print(sm.multilabel_confusion_matrix(y_test, knc_predict))

titles_options = [("Confusion matrix, without normalization", None),
                  ("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(knc, x_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize=normalize)
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)
plt.show()
TN, FP, FN, TP = confusion_matrix(y_test,knc_predict).ravel()
TPR = TP/(TP+FN)
TNR = TN/(TN+FP)
FPR = FP/(FP+TN)
FNR = FN/(TP+FN)
print("True Positive Rate:")
print(TPR)
print("True Negative Rate:")
print(TNR)
print("False Positive Rate:")
print(FPR)
print("False Negative Rate:")
print(FNR)


#DECISIONTREECLASSIFIER
print("DECISION_TREE_CLASSIFIER:")
DTree = DecisionTreeClassifier()
DTree.fit(x_train, y_train)
D_predict = DTree.predict(x_test)
#print(y_test.head())
#print(D_predict)
from sklearn.metrics import r2_score
acc=r2_score(y_test, D_predict)
accuracy = accuracy_score(y_test, D_predict)
recall = recall_score(y_test, D_predict , average='macro')
precision = precision_score(y_test,D_predict ,pos_label=1, average='macro', sample_weight=None, zero_division=0)
f1 = f1_score(y_test,D_predict,average="macro")
print("Decision Tree Classifier (r2 score):-")
print(acc)
print("Accuracy:")
print(accuracy)
print("Precision:")
print(precision)
print("Recall:")
print(recall)
print("F1score:")
print(f1)
#print("Confusion Matrix:")
#print(confusion_matrix(y_test, D_predict))
print("Classification Report:")
print(classification_report(y_test, D_predict))
print("Confusion Matrix(Multilabel):")
print(sm.multilabel_confusion_matrix(y_test, D_predict))
titles_options = [("Confusion matrix, without normalization", None),
                  ("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(DTree, x_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize=normalize)
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)
plt.show()
TN, FP, FN, TP = confusion_matrix(y_test,D_predict).ravel()
TPR = TP/(TP+FN)
TNR = TN/(TN+FP)
FPR = FP/(FP+TN)
FNR = FN/(TP+FN)
print("True Positive Rate:")
print(TPR)
print("True Negative Rate:")
print(TNR)
print("False Positive Rate:")
print(FPR)
print("False Negative Rate:")
print(FNR)


#RANDOMFORESTCLASSIFIER
print("RANDOMFOREST_CLASSIFIER:")
rf=RandomForestClassifier()
rf.fit(x_train,y_train)
#RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',
                     # max_depth=None, max_features='auto', max_leaf_nodes=None,
                      #max_samples=None, min_impurity_decrease=0.0,
                      #min_impurity_split=None, min_samples_leaf=1,
                      #min_samples_split=2, min_weight_fraction_leaf=0.0,
                      #n_estimators=100, n_jobs=None, oob_score=False,
                      #random_state=None, verbose=0, warm_start=False)
rfy_predict=rf.predict(x_test)
acc=r2_score(y_test,rfy_predict)
#print(y_test.head())
#print(rfy_predict)
accuracy = accuracy_score(y_test, rfy_predict)
recall = recall_score(y_test, rfy_predict,average='macro')
precision = precision_score(y_test,rfy_predict,pos_label=1, average='macro', sample_weight=None, zero_division=0)
f1 = f1_score(y_test,rfy_predict,average='macro')
print("Random Forest Classifier (r2 score):-")
print(acc)
print("Accuracy:")
print(accuracy)
print("Precision:")
print(precision)
print("Recall:")
print(recall)
print("F1score:")
print(f1)
#print("Confusion Matrix:")
#print(confusion_matrix(y_test, rfy_predict))
print("Classification Report:")
print(classification_report(y_test, rfy_predict))
print("Confusion Matrix(Multilabel):")
print(sm.multilabel_confusion_matrix(y_test, rfy_predict))
titles_options = [("Confusion matrix, without normalization", None),
                  ("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(rf, x_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize=normalize)
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)
plt.show()
TN, FP, FN, TP = confusion_matrix(y_test,rfy_predict).ravel()
TPR = TP/(TP+FN)
TNR = TN/(TN+FP)
FPR = FP/(FP+TN)
FNR = FN/(TP+FN)
print("True Positive Rate:")
print(TPR)
print("True Negative Rate:")
print(TNR)
print("False Positive Rate:")
print(FPR)
print("False Negative Rate:")
print(FNR)



#ADABOOSTCLASSIFIER
print("ADABOOST_CLASSIFIER:")
ab=ske.AdaBoostClassifier(n_estimators=100)
ab.fit(x_train, y_train)#fit may be called as 'trained'
ab_predict = ab.predict(x_test)
#print(y_test.head())
#print(ab_predict)
acc = r2_score(y_test,ab_predict)
accuracy = accuracy_score(y_test, ab_predict)
recall = recall_score(y_test, ab_predict , average="macro")
precision = precision_score(y_test,ab_predict , pos_label=1, average='macro', sample_weight=None, zero_division=0)
f1 = f1_score(y_test,ab_predict,average="macro")
#print("Adaptive boost Classifier (r2 score):-")
#print(acc)
print("Accuracy:")
print(accuracy)
print("Precision:")
print(precision)
print("Recall:")
print(recall)
print("F1score:")
print(f1)
#print("Confusion Matrix:")
#print(confusion_matrix(y_test, ab_predict))
print("Classification Report:")
print(classification_report(y_test, ab_predict))
print("Confusion Matrix(Multilabel):")
print(sm.multilabel_confusion_matrix(y_test, ab_predict))
titles_options = [("Confusion matrix, without normalization", None),
                  ("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(ab, x_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize=normalize)
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)
plt.show()
TN, FP, FN, TP = confusion_matrix(y_test,ab_predict).ravel()
TPR = TP/(TP+FN)
TNR = TN/(TN+FP)
FPR = FP/(FP+TN)
FNR = FN/(TP+FN)
print("True Positive Rate:")
print(TPR)
print("True Negative Rate:")
print(TNR)
print("False Positive Rate:")
print(FPR)
print("False Negative Rate:")
print(FNR)


"""
#GRADIENTBOOSTINGCLASSIFIER
print("GRADIENTBOOSTING_CLASSIFIER:")
GB=ske.GradientBoostingClassifier(n_estimators=50)
GB.fit(x_train, y_train)#fit may be called as 'trained'
GB_predict=GB.predict(x_test)
print(y_test.head())
print(GB_predict)
acc=r2_score(y_test,GB_predict)
accuracy = accuracy_score(y_test, GB_predict)
recall = recall_score(y_test, GB_predict , average='macro')
precision = precision_score(y_test,GB_predict , pos_label=1, average='macro', sample_weight=None, zero_division=0)
f1 = f1_score(y_test,GB_predict,average='macro')
print("Gradient Boosting Classifier (r2 score):-")
print(acc)
print("accuracy")
print(accuracy)
print("precision")
print(precision)
print("recall")
print(recall)
print("f1score")
print(f1)
print("Confusion Matrix(Multilabel):")
print(sm.multilabel_confusion_matrix(y_test, y_predict))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_predict))
print("Classification Report:")
print(classification_report(y_test, y_predict))
"""
#HISTOGRAMBOOSTINGCLASSIFIER
print("HISTOGRAMBOOSTING_CLASSIFIER:")
Hgb= HistGradientBoostingClassifier()
Hgb.fit(x_train,y_train)
hgb_predict=Hgb.predict(x_test)
#print(y_test.head())
#print(hgb_predict)
acc = r2_score(y_test,hgb_predict)
accuracy = accuracy_score(y_test, hgb_predict)
recall = recall_score(y_test, hgb_predict , average='macro')
precision = precision_score(y_test,hgb_predict , pos_label=1, average='macro', sample_weight=None, zero_division=0)
f1 = f1_score(y_test,hgb_predict,average='macro')
print("Histogram Gradient Boosting Classifier(r2_score):-")
print(acc)
print("Accuracy:")
print(accuracy)
print("Precision:")
print(precision)
print("Recall:")
print(recall)
print("F1score:")
print(f1)
#print("Confusion Matrix:")
#print(confusion_matrix(y_test, hgb_predict))
print("Classification Report:")
print(classification_report(y_test, hgb_predict))
print("Confusion Matrix(Multilabel):")
print(sm.multilabel_confusion_matrix(y_test, hgb_predict))
titles_options = [("Confusion matrix, without normalization", None),
                  ("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(Hgb, x_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize=normalize)
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)
plt.show()
TN, FP, FN, TP = confusion_matrix(y_test,hgb_predict).ravel()
TPR = TP/(TP+FN)
TNR = TN/(TN+FP)
FPR = FP/(FP+TN)
FNR = FN/(TP+FN)
print("True Positive Rate:")
print(TPR)
print("True Negative Rate:")
print(TNR)
print("False Positive Rate:")
print(FPR)
print("False Negative Rate:")
print(FNR)

""""
#XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.
from xgboost import XGBClassifier
xgb = XGBClassifier(objective='reg:squarederror')
xgb.fit(x_train,y_train)
xgb_predict=xgb.predict(x_test)
acc=r2_score(y_test,xgb_predict)
print("Extreme Gradient Boosting Classifier(r2_score):-")
print(acc)
"""

#GRADIENTBOOSTINGCLASSIFIER
print("GRADIENTBOOSTING_CLASSIFIER:")
GB=ske.GradientBoostingClassifier(n_estimators=50)
GB.fit(x_train, y_train)#fit may be called as 'trained'
GB_predict=GB.predict(x_test)
#print(y_test.head())
#print(GB_predict)
acc=r2_score(y_test,GB_predict)
accuracy = accuracy_score(y_test, GB_predict)
recall = recall_score(y_test, GB_predict , average='macro')
precision = precision_score(y_test,GB_predict , pos_label=1, average='macro', sample_weight=None, zero_division=0)
f1 = f1_score(y_test,GB_predict,average='macro')
print("Gradient Boosting Classifier (r2 score):-")
print(acc)
print("Accuracy:")
print(accuracy)
print("Precision:")
print(precision)
print("Recall:")
print(recall)
print("F1score:")
print(f1)
#print("Confusion Matrix:")
#print(confusion_matrix(y_test, GB_predict))
print("Confusion Matrix(Multilabel):")
print(sm.multilabel_confusion_matrix(y_test, GB_predict))
print("Classification Report:")
print(classification_report(y_test, GB_predict))
titles_options = [("Confusion matrix, without normalization", None),
                  ("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(GB, x_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize=normalize)
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)
plt.show()
TN, FP, FN, TP = confusion_matrix(y_test,GB_predict).ravel()
TPR = TP/(TP+FN)
TNR = TN/(TN+FP)
FPR = FP/(FP+TN)
FNR = FN/(TP+FN)
print("True Positive Rate:")
print(TPR)
print("True Negative Rate:")
print(TNR)
print("False Positive Rate:")
print(FPR)
print("False Negative Rate:")
print(FNR)